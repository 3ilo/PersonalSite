One side effect of the accelerating growth of generative AI over the last two years has been that it’s becoming harder to trust digital media. Whether it be pics on your Instagram feed or spam calls about car insurance, discerning the authenticity of these once hard-to-forge signals is getting harder, mostly because generative models have become very good at fabricating them. 
<br>
It’s true, Photoshop has existed for a long time, and even before that there were counterfeit mechanisms like faking handwritten signatures. But, generative AI has created a plethora of astonishing avenues to realistically spoof seemingly self-evident data in such a way that is unnoticeable to many people. In other words, in the not so distant future, it will become impossible to believe anything we see on the internet is authentic.
<br>
Broadly, there are two ways to address this problem. The first is to become better at identifying forged content as forged content (identifying true negatives). The second is to become better at identifying authentic content as authentic content (identifying true positives). Currently, the first option (identifying true negatives), is not very reliable. According to <a href=’https://ai.meta.com/blog/stable-signature-watermarking-generative-ai/’>this</a> blog post by Meta AI, the <em>most</em> effective existing detection methods today can identify 50% of AI generated images and has an error rate (false negatives) of 1/100.
<br>
So what about the other option? I was surprised to learn that a lot of big companies have been investing in this area. It’s called “Media Provenance”, and, as the C2PA (<a href=”https://c2pa.org/”>Coalition for Content Provenance and Authenticity</a>) says, it “refers to the basic, trustworthy facts about the origins of a piece of digital content (image, video, audio recording, document).” In other words, Media Provenance is a way to prove that a piece of media was created by a trusted entity and has not been tampered with.
<br>
How can this be achieved? It turns out, through the same mechanisms that have powered TLS and the Internet for the past 20+ years — cryptography and Certificate Authorities. The C2PA, as well as a bunch of other organizations and companies (including Microsoft, Adobe, OpenAI, etc), have proposed a protocol specification for this solution, and implemented it, called “Content Credentials”. 
<br>
Content Credentials are basically metadata attached to media which include information about the identity of the content creator, time it was created, and other details which are important to know when evaluating the authenticity of a piece of content. This metadata, along with the content itself, is signed cryptographically with a secret key and then bundled together. Altogether, this forms a “C2PA Manifest”, which is the fancy name for the concept of a Content Credential. End users can consume this manifest by checking that the signature matches the content and its metadata. If any of the data was tampered, the signature will not match.
<br>
The last open question is, who maintains the secret keys and signs manifests? The answer, generally, is a CA (Certificate Authority). Not the same CAs we rely on to provide certificates to trusted websites and embed root certificates in our very hardware and operating systems, a new type of CA which provides certificates to trusted content producers. The end user can resultantly adjust their level of trust concerning content on the internet to that of the CA which provided the certificate. The true positive rate on identifying authentic content becomes whatever percentage of media is signed, and the false positive rate is 0%.
<br>
To summarize, there is a relatively new protocol for verifying content provenance and authenticity, and it is beginning to see adoption. Going forward, it could be the only way to trust the authenticity of content on the internet, which is a scary thought.
